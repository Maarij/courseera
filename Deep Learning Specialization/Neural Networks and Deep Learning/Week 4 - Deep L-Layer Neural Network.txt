Deep L-Layer Neural Network
	Logistic Regression is shallow model
	5 hidden layers is a deep network
	L denotes in layers in network
	Notations
		n^l denotes # units in layer layer
		a^l denotes activations in layer l
		a^l = g^l(z^l)
		w^l (& b^l) denotes weights for z^l
		x = a[0]
		a^L = yhat

Forward Propagation in a Deep Network
	4 layer example:
		first layer is x = z^[1] = w^[1]*x + b^[1]
			x == a^[0]
		activation is a^[1] = g(z^[1])
			activation function depends on what layer you are at
		second layer is z^[2] = w^[2]a^[1] + b^[2]
		a^[2] = g^[2](z^[2])
		z^[4] = w^[5]a[3] + b[4]
		a^[4] = g^[4](z^[4])
	general equation:
		z^[l] = w^[l]a^[l-1]+b^[l]
		a^[l] = g^[l](z^[l])
	vectorized:
		Z[1] = W[1]A[0]+b[1]
		A[1] = g[1](Z[1])
		At end yhat = g(Z[4]) = A[4]
	
In the next video, at time 6:35, the correct formula should be:
a[l]=g[l](z[l])
Note that "a" and "z have dimensions (n[l],1)
Getting Your Matrix Dimensions Right
	z1=w1x+b^
		z1 dims in example = (n1,1)
		x is input features = (n0,1)
		w1 computes to be (3,2) or (n1,n0) or (n^l, n^l-1)
	z2=w2a1+b1
		w2 is (5,3)
		a1 = g1(z1)
	b vectors has to match w/e w#x's dimensions came out to be
		or (n^l,1) 
	dw^l = (n^l,n^l-1)
	db^l = (n^l,1)
	vectorized implementation
		Z1 = z1 through zm
		Z1 is (n^1,m)
		W1 is (n^1, n^0)
		X is (n^0,m)
		b1 is (n^1,1) gets duplicated by broadcasting to n^1,m and added elementwise
		A^l = (n^l,m)
		when l=0 A0 = X= (n0,m)
		dZ^l, dZ^l =  (n^l,m)
Why Deep Representations?
	First layer may be edges
	2nd layer groups edges together to form parts
	3rd layer may group different parts of faces
	earlier layer detects simpler functions and later composes
	in audio first layer me detect low level wave form features and that gets composed
	circuit theory
		-gates form network
Building blocks of deep neural networks
	forward
		zl=wlal-1 + b1
		al = glzl
	backward
		input da^l output da^l-1, dw^l, db^l
		dw^l gets updated as w^l = w^l - a*dw^l
		dz^l = da^l * g^l' (z^l)
		dw^l = dz^l * a^l-1
		db^l = dz^l
		da^l-1 = w^lT & dz^l
		dz^l = W^l+1 T * dz^l+1 * g^l' (z^l)
hyperparameters
	learning rate a
	# iterations
	# hidden layers L
	# hidden units n^1, n^2
	choice of activation function
	
		
		
		